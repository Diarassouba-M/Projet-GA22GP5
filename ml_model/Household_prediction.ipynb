{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "aS8dRXSZJ6aU"
      },
      "outputs": [],
      "source": [
        "# Importation des bibliothèques\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LwTdRi6s2nIX"
      },
      "outputs": [],
      "source": [
        "# Chargement des données\n",
        "df = pd.read_excel('data/Base_ISM_PARIS_301224.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "IR2M7xcC2rGY",
        "outputId": "975d8c2a-2907-4fdf-ae72-1c316bcedf35"
      },
      "outputs": [],
      "source": [
        "# Afficher les premières lignes pour vérifie\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Information générale\n",
        "print(df.info())\n",
        "print(df.describe(include='all'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vérification des valeurs manquantes\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['f0'] = df['f0'].fillna(0)\n",
        "df['bien'] = df['bien'].fillna(df['bien'].median())\n",
        "df['education'] = df['education'].fillna('non renseigné')\n",
        "df['sectinst'] = df['sectinst'].fillna('non renseigné')\n",
        "df['secteur'] = df['secteur'].fillna('non renseigné')\n",
        "df['popemp'] = df['popemp'].fillna('non renseigné')\n",
        "df['ep'] = df['ep'].fillna('non renseigné')\n",
        "df['depalim'] = df['depalim'].fillna(df['depalim'].median())\n",
        "df['cq25'] = df['cq25'].fillna(df['cq25'].mode()[0]) \n",
        "\n",
        "# Vérification après traitement des valeurs manquantes\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conversion des colonnes float en int\n",
        "df[df.select_dtypes('float').columns] = df.select_dtypes('float').astype(int)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution de l'âge\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['age'], kde=True)\n",
        "plt.title(\"Distribution de l'âge\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution de la Taille du Ménage\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sorted_hhsize = sorted(df['hhsize'].unique())\n",
        "ax = sns.countplot(data=df, x='hhsize', order=sorted_hhsize, color=\"#4C72B0\")\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2, height), ha='center', va='bottom', fontsize=10, color='black', fontweight='bold', xytext=(0, 5), textcoords='offset points')\n",
        "plt.title(\"Distribution de la Taille du Ménage\", fontsize=16, fontweight=\"bold\", pad=15)\n",
        "plt.xlabel(\"Taille du ménage (hhsize)\", fontsize=14, labelpad=10)\n",
        "plt.ylabel(\"Nombre d'observations\", fontsize=14, labelpad=10)\n",
        "plt.xticks(rotation=0, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "sns.despine(left=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relation entre deux variables( depalim vs bien)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x='depalim', y='bien', data=df)\n",
        "plt.title(\"Relation entre la dépense alimentaire et les biens\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Répartition des milieux de résidence\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='milieu', data=df)\n",
        "plt.title(\"Répartition des milieux de résidence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrice de corrélation pour les variables numériques\n",
        "numeric_vars = ['age', 'hhsize', 'depalim', 'depenseM', 'bien']\n",
        "corr_matrix = df[numeric_vars].corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Matrice de Corrélation des Variables Numériques\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_vars = ['milieu', 'strate', 'b05_region', 'P0']\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "for i, var in enumerate(categorical_vars, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    \n",
        "    sns.countplot(data=df, x=var, hue=var, palette=\"viridis\", legend=False)\n",
        "    plt.title(f\"Répartition de {var}\", fontsize=14)\n",
        "    plt.xticks(rotation=45, fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relations entre variables (Dépense Totale du Ménage par Niveau de Pauvreté)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.boxplot(x='P0', y='depenseM', data=df, hue='P0', palette=\"Set2\", dodge=False, legend=False)\n",
        "\n",
        "plt.title(\"Dépense Totale du Ménage par Niveau de Pauvreté (P0)\", fontsize=14)\n",
        "plt.xlabel(\"Niveau de Pauvreté\", fontsize=12)\n",
        "plt.ylabel(\"Dépense Totale du Ménage\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse bivariée avec des Variables Numériques\n",
        "sns.pairplot(df[numeric_vars], diag_kind=\"kde\", plot_kws={'alpha': 0.6})\n",
        "plt.suptitle(\"Pairplot des Variables Numériques\", y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identifier les colonnes non numériques\n",
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "binary_columns = [col for col in categorical_columns if df[col].nunique(dropna=True) == 2]\n",
        "multi_columns = [col for col in categorical_columns if df[col].nunique(dropna=True) > 2]\n",
        "\n",
        "print(\"Colonnes binaires sélectionnées :\", binary_columns)\n",
        "print(\"Colonnes catégorielles multinomiales sélectionnées :\", multi_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encodage\n",
        "# 1. Label Encoding pour les variables binaires\n",
        "label_encoder = LabelEncoder()\n",
        "for col in binary_columns:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# 2. One-Hot Encoding pour les variables à plusieurs catégories\n",
        "df_encoded = pd.get_dummies(df, columns=multi_columns, drop_first=True)\n",
        "print(df_encoded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalisation\n",
        "# Identifier les colonnes numériques dans le DataFrame encodé\n",
        "numeric_columns_encoded = [col for col in df_encoded.columns if col != 'P0' and np.issubdtype(df_encoded[col].dtype, np.number)]\n",
        "\n",
        "# Appliquer la normalisation uniquement aux colonnes numériques\n",
        "if numeric_columns_encoded:\n",
        "    scaler = StandardScaler()\n",
        "    df_encoded[numeric_columns_encoded] = scaler.fit_transform(df_encoded[numeric_columns_encoded])\n",
        "\n",
        "print(\"Données après normalisation :\")\n",
        "print(df_encoded.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vérifier que P0 est toujours discrète\n",
        "print(\"Types de données après normalisation :\")\n",
        "print(df_encoded.dtypes)\n",
        "\n",
        "# Vérifier les valeurs uniques de P0\n",
        "print(\"Valeurs uniques de P0 :\", df_encoded['P0'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution de la variable cible\n",
        "class_counts = df['P0'].value_counts(normalize=True)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.pie(class_counts, labels=[f\"Non Pauvre ({class_counts[0]*100:.2f}%)\", f\"Pauvre ({class_counts[1]*100:.2f}%)\"], \n",
        "        autopct='%1.1f%%', startangle=90, colors=[\"#66b3ff\", \"#ff9999\"], explode=(0.1, 0))  \n",
        "\n",
        "plt.title('Distribution de la variable cible P0 (Niveau de pauvreté)', fontsize=15)\n",
        "\n",
        "plt.axis('equal')  \n",
        "plt.show()\n",
        "\n",
        "print(\"Proportions des classes dans P0 :\")\n",
        "print(round(class_counts*100,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X = df_encoded.drop(columns=[\"P0\"])\n",
        "y = df_encoded[\"P0\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Application de SMOTE pour équilibrer les classes dans l'ensemble d'entraînement\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "class_counts_smote = y_train_res.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution de la variable cible\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "\n",
        "plt.pie(class_counts_smote, labels=[f\"Non Pauvre ({class_counts_smote[0]*100/len(y_train_res):.2f}%)\", f\"Pauvre ({class_counts_smote[1]*100/len(y_train_res):.2f}%)\"], \n",
        "        autopct='%1.1f%%', startangle=90, colors=[\"#66b3ff\", \"#ff9999\"], explode=(0.1, 0)) \n",
        "\n",
        "plt.title('Distribution de la variable cible P0 après équilibrage avec SMOTE', fontsize=15)\n",
        "\n",
        "plt.axis('equal') \n",
        "plt.show()\n",
        "\n",
        "# Afficher les proportions des classes dans l'ensemble d'entraînement après SMOTE\n",
        "print(\"Proportions des classes dans P0 après SMOTE :\")\n",
        "print(class_counts_smote / len(y_train_res))  # Calcul des proportions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialisation des modèles avec des paramètres adaptés\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='liblinear'),  # Augmenter max_iter et changer le solver\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"SVM\": SVC(probability=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stocker les résultats des performances\n",
        "results = {}\n",
        "best_model = None\n",
        "best_f1_score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entraînement et évaluation de chaque modèle\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Entraînement du modèle : {model_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Entraîner le modèle\n",
        "        model.fit(X_train_res, y_train_res)\n",
        "        \n",
        "        # Faire des prédictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "        \n",
        "        # Calculer les métriques\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else \"N/A\"\n",
        "        \n",
        "        # Stocker les résultats\n",
        "        results[model_name] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1 Score\": f1,\n",
        "            \"ROC AUC\": roc_auc\n",
        "        }\n",
        "        \n",
        "        # Sélectionner le meilleur modèle basé sur le F1 Score\n",
        "        if f1 > best_f1_score:\n",
        "            best_f1_score = f1\n",
        "            best_model = model_name\n",
        "            best_model_instance = model\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'entraînement du modèle {model_name} : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher les résultats\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Résultats des modèles :\")\n",
        "print(results_df)\n",
        "\n",
        "print(f\"\\nMeilleur modèle : {best_model} (F1 Score : {best_f1_score})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Réoptimisation des hyperparamètres pour le meilleur modèle\n",
        "if best_model == \"Logistic Regression\":\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "elif best_model == \"Decision Tree\":\n",
        "    param_grid = {\n",
        "        'max_depth': [5, 10, 15, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "elif best_model == \"Random Forest\":\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, None],\n",
        "        'min_samples_split': [2, 5]\n",
        "    }\n",
        "elif best_model == \"SVM\":\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Optimisation avec GridSearchCV (si applicable)\n",
        "if param_grid: \n",
        "    print(f\"Optimisation des hyperparamètres pour {best_model}...\")\n",
        "    grid_search = GridSearchCV(best_model_instance, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "    best_model_instance = grid_search.best_estimator_\n",
        "    print(f\"Meilleurs hyperparamètres : {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sélection des variables importantes (si possible)\n",
        "if isinstance(best_model_instance, (RandomForestClassifier, LogisticRegression)):\n",
        "    if hasattr(best_model_instance, 'feature_importances_'):\n",
        "        feature_importances = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'Importance': best_model_instance.feature_importances_\n",
        "        }).sort_values(by='Importance', ascending=False)\n",
        "    elif hasattr(best_model_instance, 'coef_'):\n",
        "        feature_importances = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'Importance': np.abs(best_model_instance.coef_[0])\n",
        "        }).sort_values(by='Importance', ascending=False)\n",
        "    \n",
        "    # Sélectionner les variables importantes (par exemple, Importance > moyenne)\n",
        "    selected_features = feature_importances[feature_importances['Importance'] > feature_importances['Importance'].mean()]['Feature'].tolist()\n",
        "    print(f\"Variables retenues : {selected_features}\")\n",
        "else:\n",
        "    # Si le modèle ne fournit pas d'importance des variables, garder toutes les variables\n",
        "    selected_features = X.columns.tolist()\n",
        "    print(\"Le modèle ne fournit pas d'importance des variables. Toutes les variables sont conservées.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer un pipeline final avec les variables sélectionnées\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "pipeline_final = Pipeline(steps=[\n",
        "    ('selector', SelectFromModel(best_model_instance, threshold=\"mean\")) if len(selected_features) < len(X.columns) else ('passthrough', 'passthrough'),\n",
        "    ('classifier', best_model_instance)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entraîner le pipeline final\n",
        "if len(selected_features) < len(X.columns):\n",
        "    pipeline_final.fit(X_train_res[selected_features], y_train_res)\n",
        "else:\n",
        "    pipeline_final.fit(X_train_res, y_train_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Évaluer le pipeline final\n",
        "y_pred_final = pipeline_final.predict(X_test[selected_features] if len(selected_features) < len(X.columns) else X_test)\n",
        "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
        "f1_final = f1_score(y_test, y_pred_final)\n",
        "\n",
        "print(f\"Performance du pipeline final - Accuracy : {accuracy_final}, F1 Score : {f1_final}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "# Sauvegarder le modèle dans le dossier trained_model\n",
        "model_path = os.path.join(\"trained_model\", \"final_model.pkl\")\n",
        "joblib.dump(pipeline_final, model_path)\n",
        "\n",
        "print(f\"Modèle final sauvegardé à : {model_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
